{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandhinichowdary/FMML-LAB-1/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "AiREi_nEeN__",
        "outputId": "1a78b4cb-ef88-4c9f-a7fe-5101677e1994",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "metadata": {
        "id": "PcKvLqAxenZr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673be5c6-173e-43a3-fe3e-009656f54e16"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset), dataset.DESCR"
      ],
      "metadata": {
        "id": "1ZA9COXmcmhd",
        "outputId": "f4c5d265-b12b-46cb-bf25-2293852fc1e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(sklearn.utils._bunch.Bunch,\n",
              " '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block group\\n        - HouseAge      median house age in block group\\n        - AveRooms      average number of rooms per household\\n        - AveBedrms     average number of bedrooms per household\\n        - Population    block group population\\n        - AveOccup      average number of household members\\n        - Latitude      block group latitude\\n        - Longitude     block group longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606f343c-becd-463a-8f89-949b95a35f9a"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82f4aae-ec60-4961-caa9-abcf61300a1a"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'5,5 - A'\n",
        "'6,6 - B'\n",
        "'7,7 - C'\n",
        "\n",
        "\n",
        "'8,8  - A | C'\n",
        "'0,0 -B |A  - 0'\n",
        "\n",
        "'10,10  -A'\n",
        "'15,15  -C'"
      ],
      "metadata": {
        "id": "9i8FM2S5drf_",
        "outputId": "513c6c22-2c11-4bef-f32a-c0fa9ac72925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'15,15  -C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7966a11-6c66-475a-e6a8-66480cc8257d"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b0deb1-eb26-42e2-c8e5-519418fc20d8"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecab3e52-70bb-4d80-d67d-2d9f16628ecb"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer for first question"
      ],
      "metadata": {
        "id": "fxsCePnTkROl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the Percentage of Validation Set:\n",
        "\n",
        "Pros:\n",
        "Better Generalization: A larger validation set allows your model to evaluate its performance on a more diverse and representative subset of the data. This can help in better assessing the model's ability to generalize to new, unseen data.\n",
        "Reduced Overfitting: With a larger validation set, it becomes more challenging for the model to overfit to the validation data, as there is more data to learn from.\n",
        "Cons:\n",
        "Reduced Training Data: When you increase the percentage of data allocated to validation, you are reducing the amount of data available for training. This can be a problem if you have limited data to start with, as it may lead to underfitting if the training set is too small.\n",
        "Slower Training: Training deep learning models with a larger validation set can be computationally more expensive and time-consuming.\n",
        "Decreasing the Percentage of Validation Set:\n",
        "\n",
        "Pros:\n",
        "\n",
        "More Data for Training: A smaller validation set means you have more data available for training your model. This can be beneficial if you have limited data.\n",
        "Faster Training: Smaller validation sets require less computation for evaluating the model's performance during training.\n",
        "Cons:\n",
        "\n",
        "Overfitting Risk: With a smaller validation set, there is a higher risk of overfitting. The model may learn to perform well on the specific validation data but may not generalize well to new data.\n",
        "Less Reliable Evaluation: A small validation set may not provide a reliable estimate of your model's generalization performance because it may be more sensitive to the particular examples in the validation set.\n",
        "ation multiple times to obtain a more robust estimate of model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MFpgdE0ck4Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Answer for second question\n",
        "\n",
        "Bias:\n",
        "\n",
        "Smaller Validation Set: When you have a small validation set, the performance estimate you obtain from it might be biased or unreliable. This is because the validation set may not adequately represent the overall data distribution. A small validation set may lead to overfitting or underfitting in model selection.\n",
        "Larger Validation Set: A larger validation set is generally better at estimating the model's performance on unseen data, as it provides a more representative sample of your data. It can help reduce bias in your performance estimate.\n",
        "Variance:\n",
        "\n",
        "Smaller Training Set: If you have a small training set and a large validation set, the model might not learn enough from the training data, leading to high variance in model performance. In such cases, your model might perform poorly on the test set because it hasn't learned enough patterns from the data.\n",
        "Larger Training Set: A larger training set allows the model to learn more complex patterns from the data, potentially reducing variance in model performance. However, if the validation set remains small, you may still have some uncertainty in your performance estimate."
      ],
      "metadata": {
        "id": "jJY60zeFG8pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Answer for third question\n",
        "\n",
        "\n",
        "60-20-20 or 70-15-15 Split: A commonly used split ratio is to allocate 60-70% of your data to the training set, 15-20% to the validation set, and the remaining 15-20% to the test set. This allows you to have a reasonable amount of data for training, validation, and a final evaluation on unseen data.\n",
        "\n",
        "Cross-Validation: If you have a limited dataset, consider using k-fold cross-validation. In k-fold cross-validation, the data is divided into k subsets (or folds), and the training/validation process is repeated k times, with each subset serving as the validation set once. This approach helps make the most of your data and provides more robust performance estimates.\n",
        "\n",
        "Stratified Split: If your dataset has class imbalance or other important characteristics, consider using stratified sampling to ensure that each subset (training, validation, test) maintains the same class distribution as the original dataset. This can help avoid biased splits.\n",
        "\n",
        "Experiment and Iterate: The choice of the validation set size should also depend on your computational resources and how much data you have available. You may need to experiment with different split ratios and cross-validation strategies to find what works best for your specific problem.\n",
        "\n",
        "Consider Data Size: For very large datasets, you may be able to allocate a smaller percentage to the validation set because there's still a substantial amount of data for training. Conversely, with very small datasets, you may need to allocate a larger percentage to validation to ensure a meaningful evaluation.\n",
        "\n",
        "Ultimately, the goal is to strike a balance where the validation set is large enough to provide a reliable estimate of model performance but not so large that it significantly reduces the amount of data available for training. Additionally, it's essential to keep in mind that the quality and representativeness of your data matter just as much as the percentage split, so make sure your data preprocessing is thorough and thoughtful.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "stIDCfLrHFzb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66e4f49-42be-4476-9cd3-1abd5797f36e"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L2QTL_w8IDPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Answer for first question\n",
        "\n",
        "Reduced Variance: By performing multiple splits of the data into training and validation subsets, you reduce the impact of data randomness on the model evaluation. It provides a more robust estimate of your model's performance by averaging over multiple validation sets.\n",
        "\n",
        "Better Generalization Assessment: Averaging over multiple folds ensures that your model is evaluated on different subsets of the data. This can help you assess how well your model generalizes across various data partitions, reducing the risk of overfitting to a specific validation set.\n",
        "\n",
        "More Representative Performance Estimate: Averaging allows you to obtain a performance estimate that is less dependent on the particular random split of your data. It provides a more representative assessment of your model's expected performance on unseen data.\n",
        "\n",
        "Enhanced Confidence: Calculating statistics like the mean, standard deviation, or confidence intervals of the validation accuracy across folds can give you a better understanding of the range of possible model performances, helping you make more informed decisions.\n",
        "\n",
        "Fairer Model Selection: When comparing different models or hyperparameter settings, using cross-validation and averaging the results ensures a fairer comparison. It minimizes the chances of selecting a model that happened to perform well on a single validation split due to luck.\n",
        "\n",
        "However, it's important to note that k-fold cross-validation and averaging come with a computational cost, as you need to train and evaluate your model multiple times. The choice of the number of folds (k) depends on your dataset size, available computational resources, and the desired level of precision in your performance estimate. Common values for k include 5-fold, 10-fold, and even leave-one-out cross-validation (k equals the number of data points).\n",
        "\n",
        "In summary, averaging validation accuracy across multiple splits, such as in k-fold cross-validation, is a valuable technique for obtaining more consistent and reliable model performance estimates, reducing the impact of random variations in the data split. It is particularly useful when assessing and comparing models in machine learning tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qbBD1nUrIGsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Answer for second question\n",
        "\n",
        "\n",
        "Cross-Validation Accuracy: When you perform k-fold cross-validation, you divide your dataset into k subsets or folds. You train and validate your model k times, each time using a different fold as the validation set while the remaining k-1 folds are used for training. You then average the accuracy or other performance metrics across these k iterations. This cross-validation accuracy provides a more accurate estimate of how well your model is likely to perform on unseen data compared to a single validation split.\n",
        "\n",
        "Test Accuracy: The \"test accuracy\" typically refers to the performance of your trained model on a completely separate and previously unseen dataset, which is often kept aside until the final evaluation stage. This test dataset should not have been used during model development or hyperparameter tuning. Test accuracy provides an estimate of how well your model generalizes to real-world, out-of-sample data.\n",
        "\n",
        "While cross-validation accuracy is a valuable and more reliable estimate of your model's performance compared to a single validation split, it is not the same as test accuracy. Test accuracy remains the ultimate measure of your model's real-world generalization performance.\n",
        "\n",
        "Cross-validation is useful for model selection, hyperparameter tuning, and gaining confidence in your model's performance estimate. However, it's essential to remember that cross-validation results are still based on the same dataset, and they may not fully capture all possible variations in real-world data.\n",
        "\n",
        "In practice, it's a good idea to use cross-validation to assess and fine-tune your model, and then, once you have a final model configuration, evaluate it on a separate test dataset to obtain a more accurate estimate of its true generalization performance. This separation of a test set from the development process helps ensure that you are not overfitting to the validation data.\n"
      ],
      "metadata": {
        "id": "oI7xXPgrIh7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Answer for third question\n",
        "\n",
        "Bias-Variance Trade-off:\n",
        "\n",
        "More Iterations (Higher k): Increasing the number of iterations (k-fold cross-validation with larger k) typically reduces the bias in your performance estimate. With more folds, you get to use more of your data for validation, which can lead to a more representative estimate.\n",
        "Fewer Iterations (Lower k): Using fewer iterations (smaller k) can introduce bias because each validation fold represents a larger portion of your data. This can lead to a more stable but potentially higher variance estimate.\n",
        "Computational Cost:\n",
        "\n",
        "More Iterations: Using a higher number of iterations can be computationally expensive, especially when you have a large dataset or complex model. It may not always be feasible to use a very high k.\n",
        "Dataset Size:\n",
        "\n",
        "Small Dataset: In cases where you have a small dataset, increasing the number of iterations (higher k) can be beneficial to make the most of your limited data.\n",
        "Large Dataset: With a large dataset, you may still obtain a reliable estimate with a smaller number of iterations (lower k), and this can save computational resources.\n",
        "Stability of Estimates:\n",
        "\n",
        "More Iterations: Increasing the number of iterations can provide a more stable estimate of model performance. The average performance across more iterations is less susceptible to randomness in the data splits.\n",
        "Fewer Iterations: With fewer iterations, the estimate may be more sensitive to the particular random splits, potentially leading to variability in your results.\n",
        "In summary, the choice of the number of iterations in cross-validation should be a balance between obtaining a reliable estimate of model performance and the computational cost. A common practice is to use 5-fold or 10-fold cross-validation, as they strike a reasonable balance for many datasets. However, if you have a small dataset, you might consider using leave-one-out cross-validation (k equals the number of data points) for a more thorough evaluation. Conversely, for large datasets, you can use smaller values of k to save computational resources.\n",
        "\n",
        "It's essential to keep in mind that while more iterations can reduce bias in your estimate, they do not guarantee a better model. The quality of your data, the choice of model, and other factors also play a crucial role in model performance.\n"
      ],
      "metadata": {
        "id": "Fwuec-ubJZbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yVgSgZWDJ4cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Answer for fourth question\n",
        "\n",
        "\n",
        "Mitigating Bias:\n",
        "\n",
        "Small Training Dataset: Increasing the number of iterations in cross-validation can provide the model with more opportunities to see different subsets of the small training dataset. This can help reduce bias in the model's performance estimate, as it gets exposed to more variations in the data.\n",
        "Small Validation Dataset: When you have a small validation dataset, increasing the iterations allows you to assess the model's performance on different subsets of the validation data, reducing the impact of randomness in the data split and potentially providing a more stable estimate.\n",
        "Limitations:\n",
        "\n",
        "Extremely Small Datasets: If your training or validation dataset is extremely small (e.g., just a few data points), there are inherent limitations to how much information can be extracted from such data. Increasing the number of iterations helps, but it cannot compensate for the lack of data.\n",
        "Overfitting: With very small training datasets, there's an increased risk of overfitting, especially if you use more iterations. The model may start memorizing the training examples rather than generalizing from them.\n",
        "Computational Cost: It's worth noting that increasing the number of iterations can become computationally expensive, especially if you have limited computational resources. You should carefully consider the trade-off between computational cost and the benefits of cross-validation.\n",
        "\n",
        "In cases of extremely small datasets, you might want to explore alternative strategies:\n",
        "\n",
        "Data Augmentation: If applicable, data augmentation techniques can artificially increase the effective size of your dataset by generating additional training examples with variations of the existing data.\n",
        "Transfer Learning: Leveraging pretrained models (transfer learning) can be effective when you have limited data. You can fine-tune a pretrained model on your small dataset to achieve better results.\n",
        "Simpler Models: Consider using simpler, less complex models that are less prone to overfitting when data is scarce.\n",
        "Collect More Data: If possible, collecting more data should be a priority, as it addresses the root cause of the small dataset problem.\n",
        "While increasing the number of iterations in cross-validation can help improve the stability of your model evaluation and reduce bias, it doesn't replace the need for an adequate amount of data to train a robust model. The best approach often involves a combination of techniques tailored to the specific problem and dataset constraints.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T5B8-baFKZCY"
      }
    }
  ]
}